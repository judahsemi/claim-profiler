{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(\"../\")\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path = [BASE_DIR] + sys.path if BASE_DIR not in sys.path else sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(BASE_DIR + \"/data/nltk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess, evaluate, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(BASE_DIR + \"/saves/data/clean_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = data_dict[\"train_dataset\"].copy()\n",
    "\n",
    "# random.seed(32)\n",
    "train_data.extend(random.sample(data_dict[\"train_fdataset\"].copy(), 3100))\n",
    "\n",
    "vocab = data_dict[\"vocab\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Data length:\", len(train_data))\n",
    "print(\"Vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*train_data[:3], sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_ix = {t:i for i,t in enumerate(vocab)}\n",
    "ix_to_token = {i:t for t,i in token_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_as_str, _map, vote_map):\n",
    "        self.vote_map = vote_map\n",
    "        \n",
    "        self.data_as_int = []\n",
    "        self.max_seqlen = float(\"-inf\")\n",
    "        self.min_seqlen = float(\"inf\")\n",
    "        \n",
    "        # Convert data to integers\n",
    "        for i, dt in enumerate(data_as_str):\n",
    "            dt_as_int = dt.copy()\n",
    "            dt_as_int[\"sentence\"] = evaluate.keys_to_values(dt[\"sentence\"], _map,\n",
    "                _map[\"</unk>\"])\n",
    "            \n",
    "            self.data_as_int.append(dt_as_int)\n",
    "            self.max_seqlen = max(self.max_seqlen, len(dt_as_int[\"sentence\"]))\n",
    "            self.min_seqlen = min(self.min_seqlen, len(dt_as_int[\"sentence\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_as_int)\n",
    "    \n",
    "    def get_item_with_id(self, _id):\n",
    "        for dp in self.data_as_int[abs(_id-5):]+self.data_as_int[:abs(_id+5)]:\n",
    "            if dp[\"id\"] == _id:\n",
    "                return dp\n",
    "        # return should have been None but i'm too lazy\n",
    "        return random.choice(self.data_as_int)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        # get data sample at index=ix\n",
    "        item_1 = self.data_as_int[ix]\n",
    "        PAD_ix = token_to_ix[\"</pad>\"]\n",
    "        \n",
    "        choose_relative = False\n",
    "        \n",
    "        # print(item_1[\"label\"], ix, end=\" \")\n",
    "        \n",
    "        # if sentence is a claim and has relatives\n",
    "        if item_1[\"label\"] == \"CLAIM\" and len(item_1[\"related\"]) > 0:\n",
    "            choose_relative = bool(np.random.choice([0, 1], p=[0.15, 0.85]))\n",
    "            \n",
    "            if choose_relative:\n",
    "                p = [(6*1 if r[\"label\"] == \"REFUTES\" else (\n",
    "                        2*4 if r[\"label\"] == \"SUPPORTS\" else 4)\n",
    "                     ) for r in item_1[\"related\"]]\n",
    "                p = [n/sum(p) for n in p]\n",
    "                relative = np.random.choice(item_1[\"related\"], p=p)\n",
    "                item_2 = self.get_item_with_id(relative[\"id\"])\n",
    "                \n",
    "            else:\n",
    "                choose_relative = False\n",
    "                while True:\n",
    "                    item_2 = random.choice(self.data_as_int)\n",
    "                    if item_2[\"id\"] not in [item_1[\"id\"]]+[i[\"id\"] for i in item_1[\"related\"]]:\n",
    "                        break\n",
    "            \n",
    "        # if sentence is an evidence\n",
    "        elif item_1[\"label\"] == \"EVIDENCE\":\n",
    "            choose_relative = bool(np.random.choice([0, 1], p=[0.15, 0.85]))\n",
    "            parent = self.get_item_with_id(item_1[\"related\"][0][\"id\"])\n",
    "            \n",
    "            if choose_relative and len(parent[\"related\"]) > 1:\n",
    "                while True:\n",
    "                    current = random.choice(parent[\"related\"]).copy()\n",
    "                    if current[\"id\"] == item_1[\"id\"]:\n",
    "                        break\n",
    "                       \n",
    "                p = [(0 if r[\"id\"] == item_1[\"id\"] else (\n",
    "                        6*4 if r[\"label\"] == \"SUPPORTS\" else (\n",
    "                            14*1 if r[\"label\"] == \"REFUTES\" else 2)\n",
    "                     )) for r in parent[\"related\"]]\n",
    "                p = [n/sum(p) for n in p]\n",
    "                relative = np.random.choice(parent[\"related\"], p=p).copy()\n",
    "                item_2 = self.get_item_with_id(relative[\"id\"])\n",
    "                        \n",
    "                if current[\"label\"] == relative[\"label\"] and current[\"label\"] == \"NOT_ENOUGH_INFO\":\n",
    "                    relative[\"label\"] = \"NOT_ENOUGH_INFO\"\n",
    "                    \n",
    "                elif current[\"label\"] == relative[\"label\"]:\n",
    "                    relative[\"label\"] = \"SUPPORTS\"\n",
    "                    \n",
    "                else:\n",
    "                    relative[\"label\"] = \"REFUTES\"\n",
    "                    \n",
    "            else:\n",
    "                choose_relative = False\n",
    "                while True:\n",
    "                    item_2 = random.choice(self.data_as_int)\n",
    "                    if item_2[\"id\"] not in [item_1[\"id\"]]+[i[\"id\"] for i in parent[\"related\"]]:\n",
    "                        break\n",
    "                     \n",
    "        # else, for every other scenerio\n",
    "        else:\n",
    "            choose_relative = False\n",
    "            while True:\n",
    "                item_2 = random.choice(self.data_as_int)\n",
    "                if item_2[\"id\"] != item_1[\"id\"]:\n",
    "                    break\n",
    "                    \n",
    "        # get sample\n",
    "        x1_pad = item_1[\"sentence\"]\n",
    "        x1_len = len(x1_pad)\n",
    "        \n",
    "        # get sample\n",
    "        x2_pad = item_2[\"sentence\"]\n",
    "        x2_len = len(x2_pad)\n",
    "        \n",
    "        if choose_relative:\n",
    "            y_pad = [1, self.vote_map[relative[\"label\"]]]\n",
    "            \n",
    "        else:\n",
    "            y_pad = [0, 0]\n",
    "        \n",
    "        # Pad x to self.max_seqlen\n",
    "        x1_pad += ([PAD_ix] * (self.max_seqlen+1 - len(x1_pad)))\n",
    "        x2_pad += ([PAD_ix] * (self.max_seqlen+1 - len(x2_pad)))\n",
    "        \n",
    "        return (\n",
    "            (torch.tensor(x1_pad), torch.tensor(x1_len)),\n",
    "            (torch.tensor(x2_pad), torch.tensor(x2_len))\n",
    "        ), torch.tensor(y_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vote_map = {\"NOT_ENOUGH_INFO\": 0, \"SUPPORTS\": 1, \"REFUTES\": 2}\n",
    "dataset = Dataset(train_data, token_to_ix, vote_map)\n",
    "dataloader = DataLoader(dataset, 8, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size:\", len(dataset))\n",
    "print(\"Max sequence length:\", dataset.max_seqlen)\n",
    "print(\"Min sequence length:\", dataset.min_seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "var_counter = Counter()\n",
    "\n",
    "try:\n",
    "    for i, __d in enumerate(dataloader):\n",
    "        var_counter.update([\"_\".join(map(str, l)) for l in __d[1].tolist()])\n",
    "        continue\n",
    "except Exception as e:\n",
    "    print(i)\n",
    "    raise e\n",
    "    \n",
    "print(var_counter.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, _map, h_size, out_size, emb_dim=128, n_layers=2, dropout_p=0.2):\n",
    "        \"\"\" \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.vocab_size  = len(_map)\n",
    "        self.hidden_size = h_size\n",
    "        self.output_size = out_size\n",
    "        self.emb_dim     = emb_dim\n",
    "        self.n_layers    = max(n_layers, 2)\n",
    "        self.dropout_p   = dropout_p\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            embedding_dim =self.emb_dim,\n",
    "            padding_idx   =_map[\"</pad>\"])\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size =self.emb_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers =self.n_layers,\n",
    "            batch_first=True,\n",
    "            dropout    =self.dropout_p,\n",
    "            bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            in_features =self.hidden_size*4,\n",
    "            out_features=self.hidden_size)\n",
    "        \n",
    "        self.classifier = nn.Linear(\n",
    "            in_features =self.hidden_size,\n",
    "            out_features=1)\n",
    "        \n",
    "        self.multi_classifier = nn.Linear(\n",
    "            in_features =self.hidden_size,\n",
    "            out_features=self.output_size)\n",
    "        \n",
    "    def forward(self, x, prev_state, *, verbose=False):\n",
    "        \"\"\" \"\"\"\n",
    "        if verbose:\n",
    "            print(\"*\"*10, \"INPUT\", \"*\"*10)\n",
    "            print(x[0][0].shape)\n",
    "            print(x[0][1].shape)\n",
    "        n_b, n_s = x[0][0].shape\n",
    "        \n",
    "        embed = self.embedding(torch.cat([x[0][0], x[1][0]], dim=0))\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "            print(\"*\"*10, \"EMBED\", \"*\"*10)\n",
    "            print(embed.shape)\n",
    "        \n",
    "        x_len = torch.cat([x[0][1], x[1][1]], dim=0)\n",
    "\n",
    "        embed = nn.utils.rnn.pack_padded_sequence(embed, x_len, True, False)\n",
    "        yhat, state = self.lstm(embed, None)\n",
    "        yhat, _ = nn.utils.rnn.pad_packed_sequence(yhat, True, total_length=n_s)\n",
    "        \n",
    "        yhat = yhat.view(n_b*2, n_s, 2, self.hidden_size)\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "            print(\"*\"*10, \"Sanity check: Last timestep\", \"*\"*10)\n",
    "            print(yhat.shape)\n",
    "            # print(\"should be zero (0):\", yhat[range(n_b*2), x_len, :].sum())\n",
    "            # print(yhat[range(n_b), x[1]-1, :].shape)\n",
    "            print(\"should not be zero (0):\", yhat[range(n_b*2), x_len-1, :].sum())\n",
    "            \n",
    "        yhat = torch.cat([yhat[range(n_b*2), x_len-1, 0, :],\n",
    "                          yhat[range(n_b*2), x_len*0, 1, :]], dim=-1) / x_len.view(n_b*2, 1)\n",
    "        yhat = torch.cat([yhat[:n_b], yhat[n_b:]], dim=-1)\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "            print(\"*\"*10, \"Y_HAT\", \"*\"*10)\n",
    "            print(yhat.shape)\n",
    "            # print(yhat)\n",
    "        \n",
    "        yhat = self.dropout(yhat)\n",
    "        yhat = self.fc(yhat)\n",
    "        out = self.classifier(yhat)\n",
    "        out_cls = self.multi_classifier(yhat)\n",
    "        if verbose:\n",
    "            print(\"\\n\")\n",
    "            print(\"*\"*10, \"OUTPUT\", \"*\"*10)\n",
    "            print(out.shape)\n",
    "            print(out_cls.shape)\n",
    "            print([s.shape for s in state])\n",
    "        return (out, out_cls), state\n",
    "    \n",
    "    def init_state(self, b_size=1):\n",
    "        return (torch.zeros(self.n_layers, b_size, self.hidden_size),\n",
    "                torch.zeros(self.n_layers, b_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Model(token_to_ix, 16, 3, emb_dim=32, n_layers=2, dropout_p=0.2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "__out = model(__d[0], None, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    m_data = torch.load(path)\n",
    "    \n",
    "    m = Model(\n",
    "        _map      =m_data[\"_map\"],\n",
    "        h_size    =m_data[\"hidden_size\"],\n",
    "        out_size  =m_data[\"output_size\"],\n",
    "        emb_dim   =m_data[\"emb_dim\"],\n",
    "        n_layers  =m_data[\"n_layers\"],\n",
    "        dropout_p =m_data[\"dropout_p\"])\n",
    "    \n",
    "    m.load_state_dict(m_data[\"state_dict\"])\n",
    "    l_hist = m_data[\"loss_history\"]\n",
    "    return m, l_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment cell to load the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, loss_history = load_model(BASE_DIR + \"/saves/model/r-vs-nonr-classifier.pt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(m, l_hist, _map, path=None):\n",
    "    if not path: path = BASE_DIR + \"/saves/model/r-vs-nonr-classifier.pt\"\n",
    "        \n",
    "    m_data = {\n",
    "        \"_map\"        : _map,\n",
    "        \"hidden_size\" : m.hidden_size,\n",
    "        \"emb_dim\"     : m.emb_dim,\n",
    "        \"output_size\"  : m.output_size,\n",
    "        \"n_layers\"    : m.n_layers,\n",
    "        \"dropout_p\"   : m.dropout_p,\n",
    "        \"state_dict\"  : m.state_dict(),\n",
    "        \"loss_history\": l_hist}\n",
    "    torch.save(m_data, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "ce_loss = nn.CrossEntropyLoss(ignore_index=token_to_ix[\"</pad>\"], reduction=\"none\")\n",
    "\n",
    "def criterion(out, y):\n",
    "    cls_loss = bce_loss(out[0], y[:, [0]].float())\n",
    "\n",
    "    cat_loss = ce_loss(out[1], y[:, 1])\n",
    "    cat_loss = (cat_loss * y[:, [0]]).sum() / y[:, [0]].sum()\n",
    "    return cls_loss + cat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iteration =200\n",
    "per_iter = 20\n",
    "start_t = time.time()\n",
    "\n",
    "for _ti in range(iteration//per_iter):\n",
    "    model, costs = training.train(\n",
    "        model, dataloader, per_iter, criterion, print_every=5,\n",
    "        sleep=20, sleep_every=5)\n",
    "    \n",
    "    loss_history.extend(costs)\n",
    "    save_model(model, loss_history, token_to_ix)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Round: {:2} of {:2}, Running Time: {:7.2f} sec\".format(\n",
    "        _ti+1, iteration//per_iter, time.time() - start_t))\n",
    "    print(\"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cum = 20\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.plot(\n",
    "    [sum(loss_history[i:i+cum])/cum for i in range(0, len(loss_history), cum)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Iter: {} | Min: {:.4f} | Max: {:.4f} | Last: {:.4f} | Ave: {:.4f}\".format(\n",
    "    len(loss_history), min(loss_history), max(loss_history), loss_history[-1],\n",
    "    sum(loss_history)/len(loss_history)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y_true, var_y_pred = [], []\n",
    "\n",
    "for i, __dd in enumerate(dataloader):\n",
    "    var_y_true.extend(\n",
    "        (__dd[1][:, [0]] * (__dd[1][:, [1]] + 1)).flatten().tolist())\n",
    "    \n",
    "    var_out = model(__dd[0], None)[0]\n",
    "    var_y_pred.extend((\n",
    "        (torch.sigmoid(var_out[0]) >= 0.5) *\\\n",
    "        (torch.topk(var_out[1], k=1, dim=-1)[1] + 1)\n",
    "    ).flatten().tolist())\n",
    "    \n",
    "    if i == 3:\n",
    "        break\n",
    "        \n",
    "print(\"y true:\", len(var_y_true), \"; y pred:\", len(var_y_pred))\n",
    "print(\"y true class count:\", Counter(var_y_true))\n",
    "print(\"y pred class count:\", Counter(var_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:  \", metrics.accuracy_score(var_y_true, var_y_pred))\n",
    "print(\"Precision: \", metrics.precision_score(var_y_true, var_y_pred, average=None))\n",
    "print(\"Recall:    \", metrics.recall_score(var_y_true, var_y_pred, average=None))\n",
    "print(\"F1 Score:  \", metrics.f1_score(var_y_true, var_y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(\n",
    "    var_y_true, var_y_pred, normalize=\"true\"))\n",
    "\n",
    "plt.imshow(metrics.confusion_matrix(\n",
    "    var_y_true, var_y_pred, normalize=\"true\"), cmap=plt.cm.gray_r)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
