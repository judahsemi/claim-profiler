{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import json\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.functional import F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(\"../\")\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path = [BASE_DIR] + sys.path if BASE_DIR not in sys.path else sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(BASE_DIR + \"/data/nltk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import preprocess, evaluate, training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Climate FEVER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with codecs.open(BASE_DIR + \"/data/climate-fever.json\", encoding=\"utf-8\") as fp:\n",
    "    data = json.loads(fp.read())\n",
    "    \n",
    "print(len(data))\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_clabel_count = {} # Counter([d[\"claim_label\"] for d in data])\n",
    "var_elabel_count = {}\n",
    "var_evote_count = {}\n",
    "\n",
    "for var_d in data:\n",
    "    if not var_d[\"claim_label\"] in var_clabel_count:\n",
    "        var_clabel_count[var_d[\"claim_label\"]] = 0\n",
    "    var_clabel_count[var_d[\"claim_label\"]] += 1\n",
    "    \n",
    "    for var_evid in var_d[\"evidences\"]:\n",
    "        if var_evid[\"evidence_label\"] not in var_elabel_count:\n",
    "            var_elabel_count[var_evid[\"evidence_label\"]] = 0\n",
    "        var_elabel_count[var_evid[\"evidence_label\"]] += 1\n",
    "    \n",
    "        for var_vote in var_evid[\"votes\"]:\n",
    "            if var_vote not in var_evote_count:\n",
    "                var_evote_count[var_vote] = 0\n",
    "            var_evote_count[var_vote] += 1\n",
    "\n",
    "print(\"Claim labels:\\n{}\".format(var_clabel_count))\n",
    "print(\"\\nEvidence labels:\\n{}\".format(var_elabel_count))\n",
    "print(\"\\nEvidence Votes:\\n{}\".format(var_evote_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First FEVER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with codecs.open(BASE_DIR + \"/data/train.jsonl\", encoding=\"utf-8\") as fp:\n",
    "    fever_data = [json.loads(l) for l in fp.readlines()]\n",
    "    \n",
    "print(len(fever_data))\n",
    "fever_data[:1]\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "with codecs.open(BASE_DIR + \"/saves/data/first-fever.json\", mode=\"w\", encoding=\"utf-8\") as fp:\n",
    "    random.seed(32)\n",
    "    var_ignore = [l for l in fever_data if l[\"verifiable\"] == \"NOT VERIFIABLE\"]\n",
    "    random.shuffle(var_ignore)\n",
    "    var_choosen = var_ignore[:8000]\n",
    "    \n",
    "    random.seed(32)\n",
    "    var_verify = [l for l in fever_data if l[\"verifiable\"] == \"VERIFIABLE\"]\n",
    "    random.shuffle(var_verify)\n",
    "    var_choosen.extend(var_verify[:6500])\n",
    "    \n",
    "    random.seed(32)\n",
    "    random.shuffle(var_choosen)\n",
    "    json.dump(var_choosen, fp, indent=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(BASE_DIR + \"/saves/data/first-fever.json\", encoding=\"utf-8\") as fp:\n",
    "    fever_data = json.loads(fp.read())\n",
    "    \n",
    "print(len(fever_data))\n",
    "fever_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_fcverifi_count = Counter([d[\"verifiable\"] for d in fever_data])\n",
    "var_fclabel_count = Counter([d[\"label\"] for d in fever_data])\n",
    "\n",
    "print(\"First FEVER Dataset\\n===\\n\")\n",
    "print(\"Claim Verifiable:\\n{}\".format(var_fcverifi_count))\n",
    "print(\"\\nClaim Label:\\n{}\".format(var_fclabel_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sent(sent):\n",
    "    return \" \".join(nltk.word_tokenize(sent.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dataset = []\n",
    "var_curr_id = 0\n",
    "\n",
    "for c in data:\n",
    "    var_related_set = []\n",
    "    \n",
    "    # Add claim\n",
    "    var_curr_id += 1\n",
    "    var_related_set.append({\n",
    "        \"id\": var_curr_id,\n",
    "        \"sentence\": clean_sent(c[\"claim\"]),\n",
    "        \"label\": \"CLAIM\",\n",
    "        \"related\": []\n",
    "    })\n",
    "    \n",
    "    for e in c[\"evidences\"]:\n",
    "        # Add evidence\n",
    "        var_curr_id += 1\n",
    "        var_related_set.append({\n",
    "            \"id\": var_curr_id,\n",
    "            \"sentence\": clean_sent(e[\"evidence\"]),\n",
    "            \"label\": \"EVIDENCE\",\n",
    "            \"related\": [{\n",
    "                \"id\": var_related_set[0][\"id\"],\n",
    "                \"label\": e[\"evidence_label\"]\n",
    "            }]\n",
    "        })\n",
    "        \n",
    "        # Append evidence to claim's related sentences\n",
    "        var_related_set[0][\"related\"].append({\n",
    "            \"id\": var_curr_id,\n",
    "            \"label\": e[\"evidence_label\"]\n",
    "        })\n",
    "        \n",
    "    # Append current claim data to dataset\n",
    "    dataset.extend(var_related_set)\n",
    "    \n",
    "fdataset = []\n",
    "\n",
    "for c in fever_data:\n",
    "    # Add claim\n",
    "    var_curr_id += 1\n",
    "    fdataset.append({\n",
    "        \"id\": var_curr_id,\n",
    "        \"sentence\": clean_sent(c[\"claim\"]),\n",
    "        \"label\": \"CLAIM\" if c[\"verifiable\"] == \"VERIFIABLE\" else \"IGNORE\",\n",
    "        \"related\": []\n",
    "    })\n",
    "    \n",
    "print(\"Climate Dataset:\", Counter([d[\"label\"] for d in dataset]))\n",
    "print(\"FEVER Dataset:\", Counter([d[\"label\"] for d in fdataset]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "torch.save({\n",
    "        \"climate_fever\": dataset,\n",
    "        \"first_fever\": fdataset\n",
    "    }, BASE_DIR + \"/saves/data/dataset.pt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(BASE_DIR + \"/saves/data/dataset.pt\")[\"climate_fever\"]\n",
    "fdataset = torch.load(BASE_DIR + \"/saves/data/dataset.pt\")[\"first_fever\"]\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset[:1], \"\\n\")\n",
    "print(len(fdataset))\n",
    "print(fdataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dataset_len = len(dataset)\n",
    "\n",
    "train_dataset = dataset[:int(0.75*var_dataset_len)]\n",
    "val_dataset = dataset[int(0.75*var_dataset_len):int(0.80*var_dataset_len)]\n",
    "test_dataset = dataset[int(0.80*var_dataset_len):]\n",
    "\n",
    "assert(len(train_dataset)+len(val_dataset)+len(test_dataset) == len(dataset))\n",
    "\n",
    "print(\"Total Data Length:\", len(dataset))\n",
    "print(\"Training Data Length:\", len(train_dataset))\n",
    "print(\"Validation Data Length:\", len(val_dataset))\n",
    "print(\"Testing Data Length:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_fdataset_len = len(fdataset)\n",
    "\n",
    "train_fdataset = fdataset[:int(0.75*var_fdataset_len)]\n",
    "val_fdataset = fdataset[int(0.75*var_fdataset_len):int(0.80*var_fdataset_len)]\n",
    "test_fdataset = fdataset[int(0.80*var_fdataset_len):]\n",
    "\n",
    "assert(len(train_fdataset)+len(val_fdataset)+len(test_fdataset) == len(fdataset))\n",
    "\n",
    "print(\"Total Data Length:\", len(fdataset))\n",
    "print(\"Training Data Length:\", len(train_fdataset))\n",
    "print(\"Validation Data Length:\", len(val_fdataset))\n",
    "print(\"Testing Data Length:\", len(test_fdataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather Training claims and evidences to create vocab**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(BASE_DIR + \"/saves/data/train-sent.txt\", mode=\"w\", encoding='utf-8') as fp:\n",
    "    var_label_count = {}\n",
    "    var_word_list = []\n",
    "    \n",
    "    for s in train_dataset+train_fdataset:\n",
    "        fp.write(\"{}\\n\".format(s[\"sentence\"]))\n",
    "        \n",
    "        if s[\"label\"] not in var_label_count:\n",
    "            var_label_count[s[\"label\"]] = 0\n",
    "        \n",
    "        var_label_count[s[\"label\"]] += 1\n",
    "        var_word_list.extend(s[\"sentence\"].split())\n",
    "\n",
    "print(\"Word count:\", len(var_word_list))\n",
    "print(\"Unique Word count:\", len(set(var_word_list)))\n",
    "print(\"Label counts:\", var_label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse dataset words and characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_replace(w, ch_list, rep=\"\"):\n",
    "    for ch in ch_list:\n",
    "        w = w.replace(ch, rep)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "var_dash_forms = list(set(['-', '‐', '–', '—', '−', '‑']))\n",
    "print(\"No. of dash forms:\", len(var_dash_forms))\n",
    "\n",
    "var_quote_forms = list(set(['\\'', '\"', '‘', '’', '“', '”', '′', '″',]))\n",
    "print(\"No. of quote forms:\", len(var_quote_forms))\n",
    "\n",
    "var_other_forms = ['°', '˚', '…', '€', '£', '±']\n",
    "print(\"No. of other forms:\", len(var_other_forms))\n",
    "\n",
    "var_sp_ch = list(var_dash_forms) +\\\n",
    "    list(var_quote_forms) +\\\n",
    "    list(var_other_forms) +\\\n",
    "    list(string.punctuation)\n",
    "\n",
    "var_non_alpha = [\n",
    "    w for w in set(var_all_sent.strip().split())\\\n",
    "        if not w.isalpha() and not w.isdigit() and not w.isalnum() and\\\n",
    "        not (len(w)!=1 and str_replace(w, [*var_sp_ch]).isalnum()) and\\\n",
    "        str_replace(w, var_sp_ch)\n",
    "]\n",
    "\n",
    "print(\"\\nNo. of non-alphabetic words:\", len(var_non_alpha))\n",
    "print(var_non_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique characters:\", len(set(var_all_sent)))\n",
    "\n",
    "var_oov = [ch for ch in sorted(set(var_all_sent))\\\n",
    "      if ch not in string.ascii_lowercase+string.digits+string.whitespace+\\\n",
    "          string.punctuation+\"\".join(var_sp_ch)]\n",
    "\n",
    "print(\"\\nNo. of OOV characters:\", len(var_oov))\n",
    "print(var_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get vocab list, complete it, and save it along with training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_DIR + \"/saves/data/train-vocab.txt\", encoding=\"utf-8\") as fp:\n",
    "    var_vocab_text = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_vocab = [l.replace(\" \", \"\") for l in var_vocab_text.splitlines()[1:] if len(l.split()) == 2]\n",
    "var_vocab = sorted(var_vocab + list(set(\"\".join([\n",
    "    \"\".join(set(s[\"sentence\"])) for s in train_dataset+train_fdataset]))))\n",
    "var_vocab = [\"</eos>\"] + var_vocab + [\"</unk>\", \"</pad>\"]\n",
    "\n",
    "print(len(var_vocab))\n",
    "print(var_vocab[:10], \"...\", var_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_DIR + \"/saves/data/train-split.txt\", encoding=\"utf-8\") as fp:\n",
    "    var_train_data = [s.split()+[\"</eos>\"] if s.split()[-1]==\".\" else s.split()\\\n",
    "        for s in fp.read().splitlines()]\n",
    "\n",
    "print(len(var_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(var_train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, var_dt in enumerate(var_train_data):\n",
    "    try:\n",
    "        train_dataset[i][\"sentence\"] = var_dt\n",
    "    except:\n",
    "        train_fdataset[i-len(train_dataset)][\"sentence\"] = var_dt\n",
    "    \n",
    "print(train_dataset[0], \"\\n\")\n",
    "print(train_fdataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    \"train_dataset\": train_dataset,\n",
    "    \"train_fdataset\": train_fdataset,\n",
    "    \"vocab\": var_vocab}\n",
    "torch.save(data_dict, BASE_DIR + \"/saves/data/clean_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
